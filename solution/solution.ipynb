{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to improve Dataperformers's computer vision model in order to save our client's defect detection costs, defect detection rate and damage to both the brand's reputation and customers.\n",
    "\n",
    "In order to achieve this goal, we will be using Convolutional neural networks (aka CNNs) which have been proven excellent in pattern seeking tasks such as computer vision.\n",
    "\n",
    "This type of model requires a fixed size input, which will be attained by processing the data.\n",
    "\n",
    "We also explored the idea of building a combination of 2 models also based on CNNs :\n",
    "* One that would be trained on constant conditions images\n",
    "* One that would be trained on images with varying conditions\n",
    "\n",
    "In order to make a prediction with that approach, an image would be submitted to both models which output a continuous probability (regression) about that image's defectibility instead of a discrete decision (classification). The model with the most extreme (decisive) probability about the outcome would get the upper hand on the final decision.\n",
    "\n",
    "We ended up opting for a single CNN trained on both types of data because the 2 datasets could benefit from each other and a bigger sample size. With some preprocessing, both datasets could also be less distant from each other, which is great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files system lib\n",
    "import os, os.path\n",
    "\n",
    "# import data processing libs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# import model libs\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, datasets, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data : first look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by looking at data distributions and data format in order to find the best way to manipulate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant good images : 160, (75.82938388625593% of constant images)\n",
      "constant defected images : 51, (24.170616113744074% of constant images)\n",
      "varied good images : 440, (78.43137254901961% of varied images)\n",
      "varied defected images : 121, (21.568627450980394% of varied images)\n"
     ]
    }
   ],
   "source": [
    "# data repo\n",
    "constantGoodPath = '..\\data\\Part1Raw\\Part_1_Dataset\\Train\\Good'\n",
    "constantDefectedPath = '..\\data\\Part1Raw\\Part_1_Dataset\\Train\\Defected'\n",
    "variedGoodPath = '..\\data\\Part2Raw\\Part_2_Dataset\\Train\\Good'\n",
    "variedDefectedPath = '..\\data\\Part2Raw\\Part_2_Dataset\\Train\\Defected'\n",
    "\n",
    "# count occurrence of all instances\n",
    "constantGood = len([img for img in os.listdir(constantGoodPath)])\n",
    "constantDefected = len([img for img in os.listdir(constantDefectedPath)]) \n",
    "variedGood = len([img for img in os.listdir(variedGoodPath)])\n",
    "variedDefected = len([img for img in os.listdir(variedDefectedPath)])\n",
    "\n",
    "print(\"constant good images : {}, ({}% of constant images)\".format(constantGood, 100*constantGood/(constantGood+constantDefected)))\n",
    "print(\"constant defected images : {}, ({}% of constant images)\".format(constantDefected, 100*constantDefected/(constantGood+constantDefected)))\n",
    "print(\"varied good images : {}, ({}% of varied images)\".format(variedGood, 100*variedGood/(variedGood+variedDefected)))\n",
    "print(\"varied defected images : {}, ({}% of varied images)\".format(variedDefected, 100*variedDefected/(variedGood+variedDefected)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could expect from the case description, there is way more images of good products. A model that predicts good everything would then be biased and would overperform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the provided images are JPG encoded, which is not very friendly to work with, we must first find a way to generate a greyscale int vector from every single one of them. This is possible because the defects are not color dependant, but rather contrast dependant with the background of the coin, which we will be able to see clearly in grayscale.\n",
    "\n",
    "Grayscaling the images also greatly reduces the dimensionality of our data, making our training time vastly more reasonable (RBG encoding is 3 times 8bits int per pixels rather than 1 8bits int for grayscale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to dump grayscale processed images\n",
    "grayscaleConstantGood = '..\\data\\Part1Processed\\good\\grayscale.csv'\n",
    "grayscaleConstantDefected = '..\\data\\Part1Processed\\defected\\grayscale.csv'\n",
    "grayscaleVariedGood = '..\\data\\Part2Processed\\good\\grayscale.csv'\n",
    "grayscaleVariedDefected = '..\\data\\Part2Processed\\defected\\grayscale.csv'\n",
    "\n",
    "# grayscale constant good images\n",
    "#f = open(grayscaleConstantGood, 'a')\n",
    "#for img in os.listdir(constantGoodPath) :\n",
    "#    image = cv2.imread(constantGoodPath+'\\\\'+img)\n",
    "#    grayscaleImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY).flatten()\n",
    "#    np.savetxt(f, grayscaleImage, delimiter=\",\")\n",
    "\n",
    "# grayscale constant defected images\n",
    "#f = open(grayscaleConstantDefected, 'a')\n",
    "#for img in os.listdir(constantDefectedPath) :\n",
    "#    image = cv2.imread(constantDefectedPath+'\\\\'+img)\n",
    "#    grayscaleImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY).flatten()\n",
    "#    np.savetxt(f, grayscaleImage, delimiter=\",\")\n",
    "    \n",
    "# grayscale varied good images\n",
    "#f = open(grayscaleVariedGood, 'a')\n",
    "#for img in os.listdir(variedGoodPath) :\n",
    "#    image = cv2.imread(variedGoodPath+'\\\\'+img)\n",
    "#    grayscaleImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY).flatten()\n",
    "#    np.savetxt(f, grayscaleImage, delimiter=\",\")\n",
    "    \n",
    "# grayscale varied defected images\n",
    "#f = open(grayscaleVariedDefected, 'a')\n",
    "#for img in os.listdir(variedDefectedPath) :\n",
    "#    image = cv2.imread(variedDefectedPath+'\\\\'+img)\n",
    "#    grayscaleImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY).flatten()\n",
    "#    np.savetxt(f, grayscaleImage, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing pt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, all images are not of the same size, which is a problem for a fixed length CNN input layer.\n",
    "The defected images of the dataset seem to be much more detailed than the good ones.\n",
    "We explored 3 different solutions :\n",
    "* Downscale larger images to the size of the smallest one of the dataset.\n",
    "* Upscale coarser images to the size of the largest one of the dataset.\n",
    "* Downscale detailed images to a specific resolution and upscale coarse images to that same resolution (common ground). We determine the said resolution by selecting the value size that minimalizes the sum of changes (added pixels + removed pixels) to be made in the dataset.\n",
    "\n",
    "Having said that, a summary inspection of both datasets reveals that some images contain a lot of empty background space which could be cropped out and thus reducing the dimensionality of multiple images greatly (especially in dataset 2 where the conditions vary a lot). If done succesfully, this modification would reduce the amount of lost information with data upscaling and downscaling.\n",
    "\n",
    "Here are some good examples of said images :\n",
    "\n",
    "<table><tr><td><img src=\"img\\20191006_143830.jpg\" alt=\"img1\" style=\"width: 100px;\"/></td><td><img src=\"img\\20191006_144125.jpg\" alt=\"img1\" style=\"width: 100px;\"/></td><td><img src=\"img\\20191006_144118.jpg\" alt=\"img1\" style=\"width: 100px;\"/></td><td><img src=\"img\\20191006_144115.jpg\" alt=\"img1\" style=\"width: 100px;\"/></td></tr></table>\n",
    "\n",
    "Here are some observations :\n",
    "* The background of these unnecessary large images is usually homogeneous\n",
    "* If we look at specific rows and columns, the ones that contain only background (which we want to crop out) would have very low variance on the grayscale due to that homogeneity\n",
    "* Cropping out rows and columns that have very low variance (below a certain threshold) would leave us with only the coin left, the ideal situation\n",
    "* The only thing making this strategy viable without the help of an edge detection tool is that the center of the coin is empty on every picture. This hole makes it so that a column or row containing the coin contains some amount of background everytime, which increases the variance (we do not remove rows and columns with high variance). That garantees to not crop out any part of the coin\n",
    "\n",
    "The only thing to do is determine the amount of variance below which we delete a row or column (which is fairly easy if we are able to obtain grayscale of all images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the images cropped, it is now time to determine the dimension size to which we will scale the images.\n",
    "\n",
    "In order to achieve this, we compute the mean number of columns and the mean number of rows (floor or ceil, to see) of all images independently from their respective datasets.\n",
    "This will assure us that the amount of modification regarding removed and added pixels is minimized.\n",
    "\n",
    "The images that are smaller than the determined size will be upscaled, in opposition to larger images, which will be downscaled.\n",
    "\n",
    "Such image manipulation can be done with tools such as **scikit-image** or multiple other machine learning models trained specifically for this task.\n",
    "\n",
    "We could want to avoid dependency on others AI models.\n",
    "\n",
    "Downscaling can be implemented via multiple compression algorithms.\n",
    "\n",
    "For upscaling, some heuristics exist without the use of machine learning. See :\n",
    "https://en.wikipedia.org/wiki/Image_scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First challenge : Small training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs sometimes require thousands or millions of data vectors in order to perform at a sufficiently high level. \n",
    "Our data consists of less than 1000 images total, which is quite small of a sample.\n",
    "\n",
    "In order to fix this problem, we are going to use data augmentation for both defected and good images.\n",
    "The best part about our data is that it is not oriented, meaning an image does not need to have a specific orientation to make sense and be interpreted the same as the original.\n",
    "\n",
    "That gives us the possibility to make some changes such as **reflection** and **rotation**.\n",
    "\n",
    "Rotating all images by 45 degree once or multiple times (with white padding (grayscale value of 255) when not aligned with x or y axis) is a valid way to augment data.\n",
    "This multiplies our sample size by **8** (360/45). \n",
    "\n",
    "It is also possible to make a 180 degree reflection along x or y axis without affecting the input size.\n",
    "This multiplies our sample size by **2**.\n",
    "\n",
    "Gaussian noise can also be added to a certain degree to every image we have up to this point.\n",
    "This multiplies our sample size by **2**.\n",
    "\n",
    "We can also both increase and decrease contrast on every image (not too much) which works great on grayscale.\n",
    "This multiplies our sample size by **3**.\n",
    "\n",
    "With every previously mentionned augmentation technique applied, that gives us a multiplication factor of **96**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size after augmentation : 74112\n",
      "Good sample size after augmentation : 57600\n",
      "Defected sample size after augmentation : 16512\n"
     ]
    }
   ],
   "source": [
    "totalBefore = constantGood+constantDefected+variedGood+variedDefected\n",
    "goodBefore = constantGood+variedGood\n",
    "defectedBefore = constantDefected+variedDefected\n",
    "totalAfter = totalBefore*96\n",
    "goodAfter = goodBefore*96\n",
    "defectedAfter = defectedBefore*96\n",
    "\n",
    "print(\"Sample size after augmentation : {}\".format(totalAfter))\n",
    "print(\"Good sample size after augmentation : {}\".format(goodAfter))\n",
    "print(\"Defected sample size after augmentation : {}\".format(defectedAfter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second challenge : Imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third challenge : Understand the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract metrics from model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
